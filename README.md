# CNN FP4 Emulation ğŸ§ªğŸš€

Welcome to **cnn_fp4_emulation** â€“ a playground for exploring **4-bit floating-point (FP4) quantisation** on convolutional neural networks while keeping the whole training loop fully differentiable.  Two flavours of UNet live here:

| Model | Precision | Quantisation Path |
|-------|-----------|-------------------|
| `UNetFP16` | FP32 â†’ FP16 autocast | No quantisation â€“ a high-precision baseline âœ… |
| `UNetNVFP4` | FP32 â†’ FP4 (E2M1) emulated | Kitchen ğŸ”ª autograd fake-quant on every Conv/ConvT layer (GroupNorm & output layer stay full-precision) |

The repository is set up to **train both models sequentially** and log a *ton* of telemetry to Weights & Biases:

ğŸŸ¢ Raw FP weights  
ğŸŸ£ Int-encoded FP4 weights  
ğŸ”µ De-quantised FP4 weights  
ğŸŸ¡ Gradients

Everything is saved hierarchically under `plots/heatmaps/<model>/â€¦` so runs never overwrite each other.

---
## Quick Start âš¡
```bash
cd cnn_fp4_emulation
python -m venv env && source env/bin/activate
pip install -r requirements.txt  # make sure torch & wandb are present

# Train both models on GPU 4 with 0.25 channel scaling
python main.py \
  --models fp16 nvfp4 \
  --model_scale_factor 0.25 \
  --logf 50  # log every 50 steps to keep W&B tidy
```

Check out the resulting artefacts:
```
plots/heatmaps/
â”œâ”€â”€ fp16/
â”‚   â”œâ”€â”€ weights/
â”‚   â”‚   â””â”€â”€ prequantize/semantic_weights.{png,html,json}
â”‚   â””â”€â”€ gradients/semantic_gradients.{png,html,json}
â””â”€â”€ nvfp4/
    â”œâ”€â”€ weights/
    â”‚   â”œâ”€â”€ prequantize/
    â”‚   â”œâ”€â”€ quantized/
    â”‚   â””â”€â”€ dequantized/
    â””â”€â”€ gradients/
```

---
## What We Learned ğŸ“š
1. **Quantisation granularity matters.**  Per-tile power-of-two scaling drastically increases the number of FP4 codes actually used.
2. **GroupNorm can stay full precision** without degrading the quantised model.
3. **Logging the whole pipeline** (raw â†’ int â†’ de-q) reveals hidden bottlenecks that arenâ€™t obvious from accuracy alone.

---
## Roadmap âœ¨
- [ ] Plug in **mixed-precision gradient scaling** for the quantised path.  
- [ ] Add **activation quantisation histograms** alongside weights.  
- [ ] Experiment with **learnable scaling factors** instead of power-of-two.  
- [ ] Integrate **CUTLASS CuTe kernels ğŸ±** for a speed boost.  
- [ ] Extend to **object detection** tasks (the trainer already supports it!).

PRs & issues welcome â€“ letâ€™s push FP4 to its limits! ğŸ¤–ğŸ’¾ 